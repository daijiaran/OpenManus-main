# 快速开始指南

## 下载模型之后的操作

### 1. 验证模型是否下载成功

在终端运行：
```bash
ollama list
```

应该能看到 `llama3.2` 在列表中。

### 2. 测试 Ollama 服务

运行测试脚本验证配置：
```bash
python test_ollama_connection.py
```

这个脚本会：
- 检查配置文件是否正确
- 验证 Ollama 服务是否运行
- 测试模型是否能正常响应

### 3. 直接运行项目

如果一切正常，直接运行：
```bash
python main.py
```

然后输入你的问题或任务，AI 代理会开始工作！

## 常见问题

### Ollama 服务未运行

如果测试脚本显示无法连接，请：
1. 确保 Ollama 应用正在运行（在任务管理器中查看）
2. 或者在终端运行：`ollama serve`

### 模型未找到

如果显示模型未找到，请：
```bash
ollama pull llama3.2
```

### 第一次运行较慢

第一次使用模型时，加载模型需要一些时间，这是正常的。

## 使用示例

运行 `python main.py` 后，可以尝试：

1. **简单问题**：
   - "你好，介绍一下你自己"
   - "用 Python 写一个计算器"

2. **文件操作**：
   - "读取 workspace 目录下的文件"
   - "创建一个新文件 test.txt"

3. **数据分析**：
   - "分析这个 CSV 文件"
   - "绘制数据图表"

## 下一步

配置完成！现在可以：
- ✅ 运行 `python main.py` 开始使用
- ✅ 查看 `config/README_OLLAMA.md` 了解更多配置选项
- ✅ 尝试不同的模型（如 `qwen2.5`、`mistral` 等）

祝你使用愉快！

